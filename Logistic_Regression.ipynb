{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df191c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "np.seterr(all = 'ignore')\n",
    "\n",
    "# Using the dataset from the Perceptron Demo\n",
    "def load_data1zeroone(): # Initialize dataset 1 using 0 1 classification\n",
    "    URL_='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    data = pd.read_csv(URL_, header = None)\n",
    "    data = data[:100]\n",
    "    data[4] = np.where(data.iloc[:, -1]=='Iris-setosa', 0, 1)\n",
    "    #data = np.asmatrix(data, dtype = 'float64')\n",
    "    data = data.to_numpy()\n",
    "    #print(data)\n",
    "    features = np.insert(data[:, :-1],0,1,axis=1) # adds bias term (b)\n",
    "    labels = data[:, -1].flatten()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def load_data2zeroone(): # Initialize dataset 2 using 0 1 classification\n",
    "    data = datasets.load_breast_cancer()\n",
    "    features = np.insert(data.data,0,1,axis=1) # adds bias term (b)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, data.target, test_size=0.2, random_state=15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def load_data1negpos(): # Initialize dataset 1 using -1 1 classification\n",
    "    URL_='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    data = pd.read_csv(URL_, header = None)\n",
    "    data = data[:100]\n",
    "    data[4] = np.where(data.iloc[:, -1]=='Iris-setosa', -1, 1)\n",
    "    #data = np.asmatrix(data, dtype = 'float64')\n",
    "    data = data.to_numpy()\n",
    "    #print(data)\n",
    "    features = np.insert(data[:, :-1],0,1,axis=1) # adds bias term (b)\n",
    "    labels = data[:, -1].flatten()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def load_data2negpos(): # Initialize dataset 2 using -1 1 classification\n",
    "    data = datasets.load_breast_cancer()\n",
    "    features = np.insert(data.data,0,1,axis=1) # adds bias term (b)\n",
    "    target = np.where(data.target==0,-1,1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def load_data3():\n",
    "    data = pd.read_excel('Raisin_Dataset.xlsx')\n",
    "    data[4] = np.where(data.iloc[:, -1]=='Kecimen', 0, 1)\n",
    "    #data = np.asmatrix(data, dtype = 'float64')\n",
    "    data = data.to_numpy()\n",
    "    data = np.delete(data,7,1)\n",
    "    features = np.insert(data[:, :-1],0,1,axis=1) # adds bias term (b)\n",
    "    labels = data[:, -1].flatten()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=15)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fa59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionzeroone: # Performing Logistic Regression using 0 1 formula\n",
    "    def __init__(self, max_iters=100, learning_rate=0.001, epsilon=1e-5, lam=0.01):\n",
    "        self.max_iters = max_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.lam = lam\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        m = X.size\n",
    "        loss, i = 0, 0\n",
    "        epoch_list, recorded_loss = [], []\n",
    "        while True:\n",
    "            if (i > 0):\n",
    "                recorded_loss.append(loss)\n",
    "                epoch_list.append(i)\n",
    "            #print(np.dot(X, self.w))\n",
    "            model_prediction_vector = (1 / (1 + np.exp(-1 * np.dot(X, self.w)))) + 1e-15\n",
    "            #print(y.T)\n",
    "            #new_loss = (-1/m)*np.sum(np.dot(y.T, np.log(model_prediction_vector)) + np.dot((1-y.T),np.log(1-model_prediction_vector))) + (self.lam/2)*np.sum(np.power(self.w, 2))\n",
    "            #new_loss = (-1/m)*np.sum(1 + np.exp(-1 * np.dot(y.T, model_prediction_vector))) + (self.lam/2)*np.sum(np.power(self.w, 2))\n",
    "############new_loss = np.log(1+np.exp(-1 * np.dot(y, model_prediction_vector))) + ((self.lam/2) * np.sum(self.w ** 2))\n",
    "            #new_loss = (1 / m) * ((-1 * np.dot(y.T,np.log(model_prediction_vector))) - np.dot(1-y.T,np.log(1-model_prediction_vector))) + ((self.w/(2*m)) *np.sum(self.w**2))\n",
    "    #new_loss = (1/m) * np.matmul(-y.T, np.log(model_prediction_vector)) - np.matmul((1 -y.T), np.log(1 - model_prediction_vector)) + ((self.lam/(2*m))*np.sum(np.power(self.w, 2)))\n",
    "        #new_loss = (1/m) * np.matmul(-y.T, np.log(model_prediction_vector)) - np.matmul((1 -y.T), np.log(1 - model_prediction_vector))\n",
    "            new_loss = -np.sum(np.dot(np.log(model_prediction_vector), y) + np.dot((1 - y), np.log(1 - model_prediction_vector)))/m  + ((self.lam/2) * np.sum(self.w ** 2))\n",
    "            #gradient = np.dot(X.transpose(), model_prediction_vector - y) - (2*self.lam*self.w)\n",
    "            #gradient = np.dot(X.T, np.dot(y, (-1 * np.exp(-1 * np.dot(y, model_prediction_vector))) / (1 + np.exp(-1 * np.dot(y, model_prediction_vector))))) + ((self.lam/m) * self.w)\n",
    "            #gradient = (1/m) * np.dot(X.T, np.dot(y, (-1 * np.exp(-1 * np.dot(y, model_prediction_vector))) / (1 + np.exp(-1 * np.dot(y, model_prediction_vector))))) + ((self.lam) * self.w)\n",
    "###########gradient = np.dot(X.T, (model_prediction_vector - y)) + (2*self.lam*self.w)\n",
    "            #gradient = (-1 * np.exp(-1 * np.dot(y, model_prediction_vector))) / (1 + np.exp(-1 * np.dot(y, model_prediction_vector)))\n",
    "            gradient = (1/m) * np.dot(X.T, (model_prediction_vector - y)) + np.sum(self.lam*self.w)\n",
    "        #gradient = (1/m) * np.dot(X.T, (model_prediction_vector - y))\n",
    "            #print((1/m) * np.dot(X.T, (model_prediction_vector - y)))\n",
    "            #print(\"WITH\",(1/m) * np.dot(X.T, (model_prediction_vector - y)) + (self.lam*self.w))\n",
    "            #print(\"VAL\", (self.lam*self.w))\n",
    "            #print((self.lam*self.w/m))\n",
    "            #print(\"ACTUAL\",y)\n",
    "            #print(\"PREDICTED\", model_prediction_vector)\n",
    "            #if (i%500==0):\n",
    "            #    print(\"ACTUAL\",y)\n",
    "            #    print(\"PREDICTED\", model_prediction_vector)\n",
    "            #    print(\"EXTRAAAAA\", gradient)\n",
    "            self.w -= self.learning_rate * gradient\n",
    "            if (abs(new_loss - loss) <= self.epsilon): # additional convergence condition\n",
    "                #print(new_loss, loss)\n",
    "                #print(\"L(wt+1,bt+1)-L(wt,bt) < eps.\")\n",
    "                #print(\"Exited at the \" + str(i) + \"th iteration.\")\n",
    "                break\n",
    "            elif (i >= self.max_iters):\n",
    "                #print(\"Max iterations reached.\")\n",
    "                #print(\"Exited at the \" + str(i) + \"th iteration.\")\n",
    "                break\n",
    "            else:\n",
    "                loss = new_loss\n",
    "                i+=1\n",
    "        return recorded_loss, epoch_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        model_prediction_vector = 1 / (1 + np.exp(-1 * np.dot(X, self.w)))\n",
    "        classification_vector = []\n",
    "        for x in model_prediction_vector:\n",
    "            if (x >= 0.5):\n",
    "                classification_vector.append(1)\n",
    "            else:\n",
    "                classification_vector.append(0)\n",
    "        return classification_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c611219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionnegpos: # Performing Logistic Regression using -1 1 formula\n",
    "    def __init__(self, max_iters=100, learning_rate=0.001, epsilon=1e-5, lam=0.01):\n",
    "        self.max_iters = max_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.lam = lam\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        m = X.size\n",
    "        loss, i = 0, 0\n",
    "        epoch_list, recorded_loss = [], []\n",
    "        while True:\n",
    "            if (i > 0):\n",
    "                recorded_loss.append(loss)\n",
    "                epoch_list.append(i)\n",
    "            model_prediction_vector = (1 / (1 + np.exp(-1 * np.matmul(X, self.w))))\n",
    "            new_loss = np.log(1+np.exp(np.dot(y,model_prediction_vector))) + ((self.lam/2) * np.sum(self.w ** 2))\n",
    "            #gradient = np.dot(X.T, np.dot(y,(-1 * np.exp(-1 * np.dot(y, model_prediction_vector))) / (1 + -1 * np.exp(-1 * np.dot(y, model_prediction_vector)))))\n",
    "            gradient = -1*np.dot(X.T,((y+1)/2)-model_prediction_vector) + np.sum(self.lam*self.w/m)\n",
    "            self.w -= self.learning_rate * gradient\n",
    "            if (abs(new_loss - loss) < self.epsilon): # additional convergence condition\n",
    "                #print(new_loss, loss)\n",
    "                #print(\"L(wt+1,bt+1)-L(wt,bt) < eps.\")\n",
    "                #print(\"Exited at the \" + str(i) + \"th iteration.\")\n",
    "                break\n",
    "            elif (i >= self.max_iters):\n",
    "                #print(\"Max iterations reached.\")\n",
    "                #print(\"Exited at the \" + str(i) + \"th iteration.\")\n",
    "                break\n",
    "            else:\n",
    "                loss = new_loss\n",
    "                i+=1\n",
    "        return recorded_loss, epoch_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        model_prediction_vector = 1 / (1 + np.exp(-1 * np.dot(X, self.w)))\n",
    "        classification_vector = []\n",
    "        for x in model_prediction_vector:\n",
    "            if (x >= 0.5):\n",
    "                classification_vector.append(1)\n",
    "            else:\n",
    "                classification_vector.append(-1)\n",
    "        return classification_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9082df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create train and test split for dataset 1 with 0 1 target\n",
    "X_train, X_test, y_train, y_test = load_data1zeroone()\n",
    "\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionzeroone(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730f6171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the Loss over Epochs for the above model\n",
    "def plot_loss(epoch_list, recorded_loss):\n",
    "    plt.plot(epoch_list, recorded_loss)\n",
    "    ax = plt.gca()\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"Training Curve\", fontsize = 24)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize = 20)\n",
    "    ax.set_ylabel(\"Loss\", fontsize = 20)\n",
    "plot_loss(epoch_list, recorded_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9882e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.4875\n",
      "Test accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "# I am not overfitting or underfitting the data\n",
    "# Trying to overfit dataset 1 with 0 1 target\n",
    "X_train, X_test, y_train, y_test = load_data1zeroone()\n",
    "poly = PolynomialFeatures(22)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionzeroone(max_iters=1000, learning_rate=.001, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdd475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# I'm only able to underfit the data, so regularization wouldn't help as it is an overfitting remedy\n",
    "# Let's try again with the -1 1 formula used in class\n",
    "# Create train and test split for dataset 1 with -1 1 target\n",
    "X_train, X_test, y_train, y_test = load_data1negpos()\n",
    "\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionnegpos(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c16607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9875\n",
      "Test accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Again, I am not overfitting or underfitting the data\n",
    "# Trying to overfit dataset 1 with -1 1 target\n",
    "X_train, X_test, y_train, y_test = load_data1negpos()\n",
    "poly = PolynomialFeatures(9)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionnegpos(max_iters=1000, learning_rate=.001, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1517d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# It seems like I'm able to overfit the data\n",
    "# Performing Logistic Regression with regularization at lambda = 0.1\n",
    "regressor = LogisticRegressionnegpos(max_iters=1000, learning_rate=.001, epsilon=1e-5, lam=0.1)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9ec1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9120879120879121\n",
      "Test accuracy: 0.8508771929824561\n"
     ]
    }
   ],
   "source": [
    "# Regularization increased the test and training accuracy\n",
    "# It looks like I was able to reduce the overfitting with regularization!\n",
    "# Let's try our model with another dataset to see if the model was very biased to dataset 1\n",
    "# Create train and test split for dataset 2 with 0 1 target\n",
    "X_train, X_test, y_train, y_test = load_data2zeroone()\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionzeroone(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df73d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9230769230769231\n",
      "Test accuracy: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "# It looks like the model overfits on the dataset\n",
    "# Performing Logistic Regression with regularization at lambda = 0.1\n",
    "regressor = LogisticRegressionzeroone(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=0.1)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdf37cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6197802197802198\n",
      "Test accuracy: 0.6578947368421053\n"
     ]
    }
   ],
   "source": [
    "# Regularization increased the test and training accuracy\n",
    "# It looks like I was able to reduce the overfitting with regularization!\n",
    "# Create train and test split for dataset 2 with -1 1 target\n",
    "X_train, X_test, y_train, y_test = load_data2negpos()\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionnegpos(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4051b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.3802197802197802\n",
      "Test accuracy: 0.34210526315789475\n"
     ]
    }
   ],
   "source": [
    "# It looks like the model underfits on the data\n",
    "# Trying to overfit dataset 2 with -1 1 target\n",
    "X_train, X_test, y_train, y_test = load_data2negpos()\n",
    "poly = PolynomialFeatures(3)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "# Performing Logistic Regression with 0 regularization\n",
    "regressor = LogisticRegressionnegpos(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=0)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "123b9271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9208791208791208\n",
      "Test accuracy: 0.8508771929824561\n"
     ]
    }
   ],
   "source": [
    "# It seems like I'm able to overfit the data\n",
    "# Performing Logistic Regression with regularization at lambda = 9\n",
    "regressor = LogisticRegressionnegpos(max_iters=1000, learning_rate=.01, epsilon=1e-5, lam=9)\n",
    "recorded_loss, epoch_list = regressor.fit(X_train, y_train)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, regressor.predict(X_train)))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2fa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training accuracy and test accuracy have both increased, but the model is still overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
